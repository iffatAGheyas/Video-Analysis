{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abb0e77-1a4a-4015-887e-fa4556e60a94",
   "metadata": {},
   "source": [
    "# inference.py\n",
    "\n",
    "Self-Contained Inference Script for Video Sentiment Classification\n",
    "-------------------------------------------------------------------\n",
    "This script loads a pretrained CNN+LSTM model checkpoint (cnn_lstm_best_val.pth)\n",
    "from the Model/ folder, scans all videos in TestSet/Happy and TestSet/Angry,\n",
    "samples 10 frames from the last 5 seconds of each clip, runs inference,\n",
    "and writes:\n",
    "  1) A multi-page PDF (test_results_with_title.pdf) with a title page and one page per clip showing sampled frames plus True|Pred labels.\n",
    "  2) Individual PNG snapshots for each clip under test_results_png/.\n",
    "\n",
    "Usage:\n",
    "  1) Place this file alongside the following folder structure:\n",
    "       Video-Analysis/\n",
    "       ├── Model/\n",
    "       │   └── cnn_lstm_best_val.pth\n",
    "       ├── TestSet/\n",
    "       │   ├── Happy/    (contains .mp4/.avi happy video clips)\n",
    "       │   └── Angry/    (contains .mp4/.avi angry video clips)\n",
    "       ├── inference.py  (this script)\n",
    "  2) In your terminal or Jupyter environment, `cd` into Video-Analysis/\n",
    "  3) Run with Python 3 (make sure you have all required libraries installed):\n",
    "       python inference.py\n",
    "  4) Outputs:\n",
    "       - test_results_with_title.pdf\n",
    "       - test_results_png/clip_*.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8430d2a0-7963-4b76-b370-e6b4f647444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\Video\n",
      "Loaded model and set to evaluation mode.\n",
      "Test DataFrame:\n",
      "                                            filepath  label\n",
      "0  C:/Users/IAGhe/OneDrive/Documents/Learning/por...  happy\n",
      "1  C:/Users/IAGhe/OneDrive/Documents/Learning/por...  happy\n",
      "2  C:/Users/IAGhe/OneDrive/Documents/Learning/por...  happy\n",
      "3  C:/Users/IAGhe/OneDrive/Documents/Learning/por...  happy\n",
      "4  C:/Users/IAGhe/OneDrive/Documents/Learning/por...  happy \n",
      "Counts per label:\n",
      "label\n",
      "angry    7\n",
      "happy    6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IAGhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\IAGhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved multi‐page PDF with title page here:\n",
      "  C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\Video\\test_results_with_title.pdf\n",
      "Saved individual PNGs in folder:\n",
      "  C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\Video\\test_results_png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Verify working directory and required folders\n",
    "# -----------------------------------------------------------------------------\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "\n",
    "# Ensure Model/ checkpoint exists\n",
    "MODEL_DIR = os.path.join(cwd, \"Model\")\n",
    "CHECKPOINT_NAME = \"cnn_lstm_best_val.pth\"\n",
    "best_model_path = os.path.join(MODEL_DIR, CHECKPOINT_NAME)\n",
    "if not os.path.isfile(best_model_path):\n",
    "    print(f\"ERROR: Model checkpoint not found at: {best_model_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure TestSet/ folders exist\n",
    "TESTSET_DIR = os.path.join(cwd, \"TestSet\")\n",
    "HAPPY_DIR = os.path.join(TESTSET_DIR, \"Happy\")\n",
    "ANGRY_DIR = os.path.join(TESTSET_DIR, \"Angry\")\n",
    "if not os.path.isdir(HAPPY_DIR) or not os.path.isdir(ANGRY_DIR):\n",
    "    print(f\"ERROR: Expecting subfolders 'Happy' and 'Angry' under {TESTSET_DIR}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Define the CNN+LSTM model class (must match training-time definition)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CNN_LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, n_classes=2, hidden_dim=256, n_layers=1, pretrained=False):\n",
    "        super(CNN_LSTM_Classifier, self).__init__()\n",
    "        # Load MobileNetV2 backbone (only feature extractor)\n",
    "        backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "        self.feature_extractor = backbone.features       # outputs (batch*T, 1280, 7, 7)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))       # collapse (1280,7,7) -> (1280,1,1)\n",
    "        self.feat_dim = backbone.last_channel             # 1280 for MobileNetV2\n",
    "\n",
    "        # Freeze backbone\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # LSTM over per-frame features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feat_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final FC layer: hidden_dim -> n_classes\n",
    "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: tensor of shape (batch_size, n_frames, 3, 224, 224)\n",
    "        returns: logits of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.size()                           # e.g. (1, 10, 3, 224, 224)\n",
    "        x = x.view(B * T, C, H, W)                         # -> (B*T, 3, 224, 224)\n",
    "        feat = self.feature_extractor(x)                   # -> (B*T, 1280, 7, 7)\n",
    "        feat = self.avgpool(feat).view(B * T, -1)          # -> (B*T, 1280)\n",
    "        feat = feat.view(B, T, self.feat_dim)              # -> (B, T, 1280)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(feat)              # -> (B, T, hidden_dim)\n",
    "        last_hidden = lstm_out[:, -1, :]                    # -> (B, hidden_dim)\n",
    "        logits = self.fc(last_hidden)                       # -> (B, n_classes)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Instantiate model, load checkpoint, set to eval\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_LSTM_Classifier(n_classes=2, hidden_dim=256, n_layers=1, pretrained=False)\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded model and set to evaluation mode.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Build a DataFrame of test clips\n",
    "# -----------------------------------------------------------------------------\n",
    "rows = []\n",
    "for label_name, folder in [(\"happy\", HAPPY_DIR), (\"angry\", ANGRY_DIR)]:\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.lower().endswith((\".mp4\", \".avi\")):\n",
    "            full_path = os.path.join(folder, fname).replace(\"\\\\\", \"/\")\n",
    "            rows.append({\"filepath\": full_path, \"label\": label_name})\n",
    "\n",
    "df_testser = pd.DataFrame(rows)\n",
    "if df_testser.empty:\n",
    "    print(\"ERROR: No video files found in TestSet/Happy or TestSet/Angry.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Test DataFrame:\")\n",
    "print(df_testser.head(), \"\\nCounts per label:\")\n",
    "print(df_testser[\"label\"].value_counts())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Define VideoDatasetInference class\n",
    "# -----------------------------------------------------------------------------\n",
    "class VideoDatasetInference(Dataset):\n",
    "    \"\"\"\n",
    "    Samples exactly n_frames from the last sample_last_secs seconds of each video.\n",
    "    Returns a tensor of shape (n_frames, 3, 224, 224) and the label index.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, n_frames=10, sample_last_secs=5, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.n_frames = n_frames\n",
    "        self.sample_last_secs = sample_last_secs\n",
    "        self.transform = transform\n",
    "        self.labels = sorted(self.df[\"label\"].unique())            # [\"angry\", \"happy\"]\n",
    "        self.label2idx = {lbl: idx for idx, lbl in enumerate(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filepath = row[\"filepath\"]\n",
    "        label_str = row[\"label\"]\n",
    "        label_idx = self.label2idx[label_str]\n",
    "\n",
    "        cap = cv2.VideoCapture(filepath)\n",
    "        if not cap.isOpened():\n",
    "            # Return dummy zero tensor if video fails\n",
    "            dummy = torch.zeros((self.n_frames, 3, 224, 224))\n",
    "            return dummy, label_idx\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30.0\n",
    "        window_len = int(fps * self.sample_last_secs)\n",
    "        start_frame = max(0, total_frames - window_len)\n",
    "\n",
    "        if total_frames > 0 and start_frame < total_frames:\n",
    "            indices = np.linspace(start_frame, total_frames - 1, num=self.n_frames).astype(int)\n",
    "        else:\n",
    "            indices = np.zeros(self.n_frames, dtype=int)\n",
    "\n",
    "        frames = []\n",
    "        for fid in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(fid))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame_rgb)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "            frames.append(img)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            dummy = torch.zeros((self.n_frames, 3, 224, 224))\n",
    "            return dummy, label_idx\n",
    "\n",
    "        # Pad with last valid frame if fewer than n_frames read\n",
    "        while len(frames) < self.n_frames:\n",
    "            frames.append(frames[-1].clone())\n",
    "\n",
    "        clip_tensor = torch.stack(frames, dim=0)   # shape: (n_frames, 3, 224, 224)\n",
    "        return clip_tensor, label_idx\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Create DataLoader for inference\n",
    "# -----------------------------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "inference_dataset = VideoDatasetInference(df_testser, n_frames=10, sample_last_secs=5, transform=transform)\n",
    "inference_loader  = DataLoader(inference_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Run inference, build PDF with title page, and save individual PNGs\n",
    "# -----------------------------------------------------------------------------\n",
    "pdf_path = os.path.join(cwd, \"test_results_with_title.pdf\")\n",
    "png_folder = os.path.join(cwd, \"test_results_png\")\n",
    "os.makedirs(png_folder, exist_ok=True)\n",
    "\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    # --- Title Page ---\n",
    "    fig_title = plt.figure(figsize=(11.69, 8.27))  # A4 landscape\n",
    "    fig_title.patch.set_facecolor('white')\n",
    "    plt.axis('off')\n",
    "\n",
    "    title_str = (\n",
    "        \"Proof‐of‐Concept:\\n\"\n",
    "        \"Video Sentiment Classification\\n\"\n",
    "        \n",
    "    )\n",
    "    plt.text(0.5, 0.5, title_str,\n",
    "             ha='center', va='center',\n",
    "             fontsize=24, weight='bold')\n",
    "\n",
    "    pdf.savefig(fig_title)\n",
    "    plt.close(fig_title)\n",
    "    # --- End Title Page ---\n",
    "\n",
    "    # --- Inference Loop ---\n",
    "    with torch.no_grad():\n",
    "        for idx, (clip_tensor, label_idx) in enumerate(inference_loader):\n",
    "            clip_tensor = clip_tensor.to(device)    # (1, 10, 3, 224, 224)\n",
    "            true_label = inference_dataset.labels[label_idx.item()]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(clip_tensor)             # (1, 2)\n",
    "            pred_idx = logits.argmax(dim=1).item()\n",
    "            pred_label = inference_dataset.labels[pred_idx]\n",
    "\n",
    "            # Sampled frames (first 5 for display)\n",
    "            frames = clip_tensor.cpu().squeeze(0)    # (10, 3, 224, 224)\n",
    "            n_plot = min(5, frames.shape[0])\n",
    "\n",
    "            fig = plt.figure(figsize=(12, 3))\n",
    "            for j in range(n_plot):\n",
    "                frame_j = frames[j].permute(1, 2, 0).numpy()\n",
    "                # Un-normalize\n",
    "                frame_j = (frame_j * np.array([0.229, 0.224, 0.225]) +\n",
    "                           np.array([0.485, 0.456, 0.406]))\n",
    "                frame_j = np.clip(frame_j, 0, 1)\n",
    "\n",
    "                ax = plt.subplot(1, n_plot, j + 1)\n",
    "                ax.imshow(frame_j)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            page_title = f\"Clip {idx + 1}: True = {true_label.upper()}  |  Pred = {pred_label.upper()}\"\n",
    "            fig.suptitle(page_title, fontsize=14)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "\n",
    "            # Save to PDF\n",
    "            pdf.savefig(fig)\n",
    "\n",
    "            # Save PNG\n",
    "            png_name = f\"clip_{idx+1}_true_{true_label}_pred_{pred_label}.png\"\n",
    "            fig.savefig(os.path.join(png_folder, png_name))\n",
    "            plt.close(fig)\n",
    "\n",
    "print(f\"Saved multi‐page PDF with title page here:\\n  {pdf_path}\")\n",
    "print(f\"Saved individual PNGs in folder:\\n  {png_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4b66a-272c-424c-a4df-c9094cccb75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
