# Videoâ€Based Customer Sentiment Classification

A proofâ€ofâ€concept pipeline that classifies short video clips of customers as **happy** or **angry** using a lightweight CNNâ€‰+â€‰LSTM model in PyTorch. All code is contained in `VideoAnalysis.ipynb`, and the trained model checkpoint is stored in the `Model` folder.

---

## ğŸš€ Overview

This repository demonstrates an endâ€toâ€end workflow for:

1. **Collecting** 42 short video clips (21 â€œhappy,â€ 21 â€œangryâ€) from Storyblocks.  
2. **Preparing** a balanced dataset and performing an 80%/10%/10% stratified split into training, validation, and test sets.  
3. **Implementing** a custom PyTorch `VideoDataset` that samples 10 frames from the last 5 seconds of each clip, resizes them to 224Ã—224, and normalises to ImageNet statistics.  
4. **Building** a model that leverages an ImageNetâ€pretrained MobileNetV2 backbone (frozen) plus a lightweight LSTM for temporal aggregation.  
5. **Training** for a small number of epochs (5) and automatically saving the checkpoint with the best validation accuracy (`cnn_lstm_best_val.pth`).  
6. **Running inference** on a heldâ€out TestSet of completely new â€œhappyâ€/â€angryâ€ clips, visualising sampled frames, groundâ€truth vs. predicted labels, and saving results in a multiâ€page PDF + individual PNGs.  

> **Note:** All video clips used for training and testing were collected from Storyblocks (royaltyâ€free).  

---

## ğŸ“‚ Repository Structure

![image](https://github.com/user-attachments/assets/f3721a16-43ab-40f8-a7ea-50fecd714cd5)

- **`VideoAnalysis.ipynb`**  
  The main Jupyter notebook with all code cells:  
  1. Imports & environment setup  
  2. Generate `labels.csv`  
  3. Split data into train/validation/test  
  4. Define `VideoDataset` and DataLoaders  
  5. Define CNNâ€‰+â€‰LSTM model, loss, and optimizer  
  6. Train & validate, saving the bestâ€val checkpoint  
  7. Inference on TestSet using the bestâ€val model, visualize & save results  

- **`Model/cnn_lstm_best_val.pth`**  
  The state dictionary of the trained model that achieved the highest validation accuracy.

- **`TrainingSet/Angry` and `TrainingSet/Happy`**  
  42 training clips in total (21 â€œangryâ€ + 21 â€œhappyâ€) used for model training and validation.

- **`TestSet/Angry` and `TestSet/Happy`**  
  A set of completely new video clips (not seen during training/validation) for final model inference and evaluation.

- **`Test_Results/`**  
  - **`test_results.pdf`**: A multiâ€page PDF, one page per test clip, showing the first 5 sampled frames and â€œTrue vs Predâ€ labels.  
  - **`clip_*.png`**: Individual PNG filesâ€”one figure per test clipâ€”named in the format `clip_{idx}_true_{label}_pred_{label}.png`.

- **`.ipynb_checkpoints/`**  
  Automatically generated by Jupyter; contains notebook checkpoints.

---

## ğŸ“‹ Requirements

- **Python 3.7+**  
- **PyTorch** (tested on 1.10+)  
- **torchvision**  
- **OpenCV (opencvâ€python)**  
- **Pandas**  
- **NumPy**  
- **Scikitâ€learn**  
- **Pillow**  
- **Matplotlib**

Install dependencies via pip:
```bash
pip install torch torchvision opencv-python pandas numpy scikit-learn pillow matplotlib

![image](https://github.com/user-attachments/assets/b09755d8-cc3b-4abc-901d-f0a5f042195c)

cd Video


