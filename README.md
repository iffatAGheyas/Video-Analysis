# Video‐Based Customer Sentiment Classification

A proof‐of‐concept pipeline that classifies short video clips of customers as **happy** or **angry** using a lightweight CNN + LSTM model in PyTorch. All code is contained in `VideoAnalysis.ipynb`, and the trained model checkpoint is stored in the `Model` folder.

---

## 🚀 Overview

This repository demonstrates an end‐to‐end workflow for:

1. **Collecting** 42 short video clips (21 “happy,” 21 “angry”) from Storyblocks.  
2. **Preparing** a balanced dataset and performing an 80%/10%/10% stratified split into training, validation, and test sets.  
3. **Implementing** a custom PyTorch `VideoDataset` that samples 10 frames from the last 5 seconds of each clip, resizes them to 224×224, and normalises to ImageNet statistics.  
4. **Building** a model that leverages an ImageNet‐pretrained MobileNetV2 backbone (frozen) plus a lightweight LSTM for temporal aggregation.  
5. **Training** for a small number of epochs (5) and automatically saving the checkpoint with the best validation accuracy (`cnn_lstm_best_val.pth`).  
6. **Running inference** on a held‐out TestSet of completely new “happy”/”angry” clips, visualising sampled frames, ground‐truth vs. predicted labels, and saving results in a multi‐page PDF + individual PNGs.  

> **Note:** All video clips used for training and testing were collected from Storyblocks (royalty‐free).  

---

## 📂 Repository Structure

![image](https://github.com/user-attachments/assets/f3721a16-43ab-40f8-a7ea-50fecd714cd5)

- **`VideoAnalysis.ipynb`**  
  The main Jupyter notebook with all code cells:  
  1. Imports & environment setup  
  2. Generate `labels.csv`  
  3. Split data into train/validation/test  
  4. Define `VideoDataset` and DataLoaders  
  5. Define CNN + LSTM model, loss, and optimizer  
  6. Train & validate, saving the best‐val checkpoint  
  7. Inference on TestSet using the best‐val model, visualize & save results  

- **`Model/cnn_lstm_best_val.pth`**  
  The state dictionary of the trained model that achieved the highest validation accuracy.

- **`TrainingSet/Angry` and `TrainingSet/Happy`**  
  42 training clips in total (21 “angry” + 21 “happy”) used for model training and validation.

- **`TestSet/Angry` and `TestSet/Happy`**  
  A set of completely new video clips (not seen during training/validation) for final model inference and evaluation.

- **`Test_Results/`**  
  - **`test_results.pdf`**: A multi‐page PDF, one page per test clip, showing the first 5 sampled frames and “True vs Pred” labels.  
  - **`clip_*.png`**: Individual PNG files—one figure per test clip—named in the format `clip_{idx}_true_{label}_pred_{label}.png`.

- **`.ipynb_checkpoints/`**  
  Automatically generated by Jupyter; contains notebook checkpoints.

---

## 📋 Requirements

- **Python 3.7+**  
- **PyTorch** (tested on 1.10+)  
- **torchvision**  
- **OpenCV (opencv‐python)**  
- **Pandas**  
- **NumPy**  
- **Scikit‐learn**  
- **Pillow**  
- **Matplotlib**

Install dependencies via pip:
```bash
pip install torch torchvision opencv-python pandas numpy scikit-learn pillow matplotlib

![image](https://github.com/user-attachments/assets/b09755d8-cc3b-4abc-901d-f0a5f042195c)

cd Video


